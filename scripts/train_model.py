import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np


# Step 1: Prepare Data
def prepare_data(data_path):
    df = pd.read_csv(data_path)
    df['datetime'] = pd.to_datetime(df['datetime'])

    # Create 'season' feature
    df['season'] = df['datetime'].dt.month % 12 // 3 + 1
    df['day_of_year'] = df['datetime'].dt.dayofyear

    # Additional Time-based features
    df['day_of_week'] = df['datetime'].dt.dayofweek
    df['month'] = df['datetime'].dt.month
    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)

    # Interaction features
    df['temp_wind_interaction'] = df['temp_mean'] * df['wind_speed_mean']
    df['humidity_pressure_interaction'] = df['humidity_mean'] * df['pressure_mean']
    df['temp_humidity_interaction'] = df['temp_mean'] * df['humidity_mean']
    df['pressure_cloud_interaction'] = df['pressure_mean'] * df['clouds_all_mean']

    # Lag features (AQI and weather variables)
    df['AQI_lag_1'] = df['AQI Value'].shift(1)
    df['AQI_lag_2'] = df['AQI Value'].shift(2)
    df['AQI_lag_3'] = df['AQI Value'].shift(3)
    df['temp_lag_1'] = df['temp_mean'].shift(1)
    df['temp_lag_2'] = df['temp_mean'].shift(2)
    df['humidity_lag_1'] = df['humidity_mean'].shift(1)
    df['humidity_lag_2'] = df['humidity_mean'].shift(2)

    # Rolling averages
    df['temp_mean_7d_avg'] = df['temp_mean'].rolling(window=7).mean()
    df['humidity_mean_7d_avg'] = df['humidity_mean'].rolling(window=7).mean()
    df['temp_mean_30d_avg'] = df['temp_mean'].rolling(window=30).mean()

    # Polynomial features
    df['temp_mean_squared'] = df['temp_mean'] ** 2
    df['wind_speed_cubed'] = df['wind_speed_mean'] ** 3

    # Fourier Series for cyclical features
    df['fourier_temp'] = np.sin(2 * np.pi * df['day_of_year'] / 365)
    df['fourier_wind'] = np.cos(2 * np.pi * df['day_of_year'] / 365)

    # Ensure numeric data types, replace non-numeric values like '.' with NaN
    df = df.apply(pd.to_numeric, errors='coerce')

    # Drop NaNs generated by rolling, lag features, and non-numeric replacements
    df = df.dropna(subset=['AQI Value', 'temp_mean_7d_avg', 'AQI_lag_1'])

    # Select relevant features including the new engineered ones
    features = [
        'temp_mean', 'temp_max', 'temp_min', 'wind_speed_mean', 'humidity_mean',
        'pressure_mean', 'clouds_all_mean', 'season', 'day_of_year', 'day_of_week', 'is_weekend',
        'temp_wind_interaction', 'humidity_pressure_interaction', 'temp_humidity_interaction',
        'pressure_cloud_interaction', 'AQI_lag_1', 'AQI_lag_2', 'AQI_lag_3',
        'temp_lag_1', 'temp_lag_2', 'humidity_lag_1', 'humidity_lag_2',
        'temp_mean_7d_avg', 'humidity_mean_7d_avg', 'temp_mean_30d_avg',
        'temp_mean_squared', 'wind_speed_cubed', 'fourier_temp', 'fourier_wind'
    ]

    X = df[features]
    y = df['AQI Value']

    # Normalize features (only for non-tree-based models)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Return both the unscaled and scaled data
    return X, X_scaled, y

# Step 2: Split Data
def split_data(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    return X_train, X_test, y_train, y_test


def train_xgboost_with_tuning(X_train, y_train):
    param_grid = {
        'n_estimators': [100, 200, 500],
        'max_depth': [3, 6, 10],
        'learning_rate': [0.01, 0.1, 0.3],
        'min_child_weight': [1, 3, 5],
        'subsample': [0.7, 0.8, 1.0],
        'colsample_bytree': [0.7, 0.8, 1.0],
    }

    xgb = XGBRegressor(objective='reg:squarederror', random_state=42)
    xgb_random = RandomizedSearchCV(estimator=xgb, param_distributions=param_grid,
                                    n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=-1)

    xgb_random.fit(X_train, y_train)
    print("Best parameters found for XGBoost:", xgb_random.best_params_)

    return xgb_random.best_estimator_

# Step 3: Train Model with Hyperparameter Tuning using RandomizedSearchCV
def train_random_forest_with_tuning(X_train, y_train):
    # Define the expanded hyperparameter grid
    param_grid = {
        'n_estimators': [100, 500, 1000, 1500],
        'max_depth': [None, 10, 20, 30, 50],
        'min_samples_split': [2, 5, 10, 20],
        'min_samples_leaf': [1, 2, 4, 6],
        'max_features': ['sqrt', 'log2', None]  # Removed 'auto' due to previous errors
    }

    # Instantiate Random Forest Regressor
    rf = RandomForestRegressor(random_state=42)

    # Set up RandomizedSearchCV
    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid,
                                   n_iter=20, cv=5, verbose=2, random_state=42, n_jobs=-1)

    # Fit RandomizedSearchCV
    rf_random.fit(X_train, y_train)

    # Print the best parameters found by RandomizedSearchCV
    print("Best parameters found:", rf_random.best_params_)

    # Return the best model
    return rf_random.best_estimator_


# Step 4: Evaluate Model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = mse ** 0.5
    r2 = r2_score(y_test, y_pred)

    print(f"MAE: {mae}")
    print(f"RMSE: {rmse}")
    print(f"RÂ² Score: {r2}")


if __name__ == "__main__":
    data_path = '/Users/cjbergin/PycharmProjects/DenverAQIPrediction/data/merged_weather_aqi_2014_2024.csv'

    # Prepare and split data
    X_unscaled, X_scaled, y = prepare_data(data_path)

    # Split unscaled data for tree-based models
    X_train_unscaled, X_test_unscaled, y_train, y_test = split_data(X_unscaled, y)

    # Split scaled data for models requiring scaling
    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = split_data(X_scaled, y)

    # Train Random Forest (using unscaled data)
    print("Training Random Forest model with hyperparameter tuning...")
    best_rf_model = train_random_forest_with_tuning(X_train_unscaled, y_train)
    print("Best Random Forest Model training completed.")
    evaluate_model(best_rf_model, X_test_unscaled, y_test)

    # Train XGBoost (using unscaled data)
    print("Training XGBoost model...")
    xgboost_model = train_xgboost_with_tuning(X_train_unscaled, y_train)
    evaluate_model(xgboost_model, X_test_unscaled, y_test)